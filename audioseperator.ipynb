{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "need to find the best libraries for separating speech of the speakers from the video\n",
    "\n",
    "should try the following:\n",
    "\n",
    "- native pyannote\n",
    "- can try if https://replicate.com/lucataco/mvsep-mdx23-music-separation works for speakers as well - \n",
    "- https://huggingface.co/speechbrain/sepformer-wham - \n",
    "- https://github.com/facebookresearch/svoice - doesnt work cause no inference part provided\n",
    "- https://github.com/JusperLee/TDANet - \n",
    "- https://github.com/modelscope/ClearerVoice-Studio - \n",
    "\n",
    "basically want to test each of the repos on my example audio file\n",
    "\n",
    "might be better if i collect one more sample audio for testing\n",
    "\n",
    "the question is also how to structuarally represent this somehow - the simplest way is tto make a folder structure like\n",
    "\n",
    "- audio-separation-samples\n",
    "    - sample1.wav\n",
    "    - sample2.wav\n",
    "    - svoice\n",
    "        - sample1-result.wav\n",
    "        - sample2-result.wav\n",
    "    - pyannote\n",
    "        - sample1-result.wav\n",
    "        - sample2-result.wav\n",
    "    - ...\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample rate: 48.0 kHz\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "# Read the audio file\n",
    "audio_info = sf.info('audio-separation-samples/sample1.wav')\n",
    "sample_rate_khz = audio_info.samplerate / 1000\n",
    "\n",
    "print(f\"Sample rate: {sample_rate_khz} kHz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='audio_8khz.wav'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "# Load the audio file\n",
    "audio = AudioSegment.from_wav('audio-separation-samples/sample1.wav')\n",
    "\n",
    "# Convert to 16kHz\n",
    "audio_8k = audio.set_frame_rate(8000)\n",
    "\n",
    "# Export the file\n",
    "audio_8k.export('audio_8khz.wav', format='wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/maxhager/projects/pyannote-audio/TDANet\n"
     ]
    }
   ],
   "source": [
    "cd TDANet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import look2hear.models\n",
    "import torchaudio\n",
    "\n",
    "# Remove or comment out the CUDA device setting since we're using CPU\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "# Load and resample audio\n",
    "mix, sr = torchaudio.load('/Users/maxhager/projects/pyannote-audio/audio-separation-samples/sample1.wav')\n",
    "transform = torchaudio.transforms.Resample(sr, 16_000)\n",
    "mix = transform(mix)\n",
    "mix = mix.view(1, 1, -1)\n",
    "\n",
    "# Load model with weights_only=False to bypass the security restriction\n",
    "model = look2hear.models.BaseModel.from_pretrain(\n",
    "    \"JusperLee/TDANetBest-2ms-LRS2\", \n",
    "    weights_only=False  # Add this parameter\n",
    ")\n",
    "\n",
    "# Process audio without .cuda()\n",
    "est_sources = model(mix)\n",
    "\n",
    "# Save the separated audio files\n",
    "torchaudio.save(\"audio1sep.wav\", est_sources[:, 0, :].detach(), 16_000)\n",
    "torchaudio.save(\"audio2sep.wav\", est_sources[:, 1, :].detach(), 16_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/sepformer-wham' if not cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac69f0b3923046d2972513245103c98c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hyperparams.yaml:   0%|          | 0.00/1.51k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/sepformer-wham' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch masknet.ckpt: Fetching from HuggingFace Hub 'speechbrain/sepformer-wham' if not cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c9f07ed7c2e42a080135690f0984ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "masknet.ckpt:   0%|          | 0.00/113M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch encoder.ckpt: Fetching from HuggingFace Hub 'speechbrain/sepformer-wham' if not cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae7d823e395c46ea8e580afb71aca8de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "encoder.ckpt:   0%|          | 0.00/17.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch decoder.ckpt: Fetching from HuggingFace Hub 'speechbrain/sepformer-wham' if not cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6efa70fbde064003b160d36023a4bcd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "decoder.ckpt:   0%|          | 0.00/17.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: masknet, encoder, decoder\n"
     ]
    }
   ],
   "source": [
    "from speechbrain.inference.separation import SepformerSeparation as separator\n",
    "import torchaudio\n",
    "\n",
    "# Initialize model explicitly with CPU\n",
    "model = separator.from_hparams(\n",
    "    source=\"speechbrain/sepformer-wham\", \n",
    "    savedir='pretrained_models/sepformer-wham',\n",
    "    run_opts={\"device\":\"cpu\"}  # Explicitly set to CPU (you could also just omit this parameter)\n",
    ")\n",
    "\n",
    "# Load and separate audio file\n",
    "est_sources = model.separate_file(path='audio_8khz.wav')\n",
    "\n",
    "# Save the separated sources\n",
    "torchaudio.save(\"source1hat.wav\", est_sources[:, :, 0].detach(), 8000)\n",
    "torchaudio.save(\"source2hat.wav\", est_sources[:, :, 1].detach(), 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{speaker_tag=1 start_sec=0.38 end_sec=3.04}\n",
      "{speaker_tag=2 start_sec=3.07 end_sec=4.77}\n",
      "{speaker_tag=3 start_sec=4.86 end_sec=6.56}\n",
      "{speaker_tag=4 start_sec=6.59 end_sec=9.66}\n",
      "{speaker_tag=5 start_sec=9.73 end_sec=12.45}\n",
      "{speaker_tag=5 start_sec=12.93 end_sec=17.18}\n",
      "{speaker_tag=6 start_sec=17.22 end_sec=21.98}\n",
      "{speaker_tag=7 start_sec=22.14 end_sec=27.62}\n"
     ]
    }
   ],
   "source": [
    "import pvfalcon\n",
    "\n",
    "access_key = \"ee4xB9zjbJbfPDjHAuZXkMOsy0IZ/hswOfNTmv1e6u11Ve43L39thA==\"\n",
    "\n",
    "falcon = pvfalcon.create(access_key)\n",
    "\n",
    "segments = falcon.process_file(\"output.wav\")\n",
    "for segment in segments:\n",
    "    print(\n",
    "        \"{speaker_tag=%d start_sec=%.2f end_sec=%.2f}\"\n",
    "        % (segment.speaker_tag, segment.start_sec, segment.end_sec)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
